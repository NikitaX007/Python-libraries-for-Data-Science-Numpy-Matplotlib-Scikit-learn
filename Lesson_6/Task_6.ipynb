{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOkO+nRgaGECExUFyWxuJPJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Библиотеки Python для Data Science: Numpy, Matplotlib, Scikit-learn"],"metadata":{"id":"e-_nlJu5LnwO"}},{"cell_type":"markdown","source":["## Урок 6. Видеоурок. Обучение с учителем в Scikit-learn"],"metadata":{"id":"PxtPlNXMLz7n"}},{"cell_type":"markdown","source":["### Тема “Обучение с учителем”\n"],"metadata":{"id":"8NFyQhxJL7fu"}},{"cell_type":"markdown","source":["Задание 1:\n","Импортируйте библиотеки pandas и numpy.\n","Загрузите \"Boston House Prices dataset\" из встроенных наборов данных библиотеки sklearn..\n","Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью\n","функции train_test_split так, чтобы размер тестовой выборки\n","составлял 30% от всех данных, при этом аргумент random state должен быть равен 42.\n","Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля\n","sklearn.linear_model.\n","Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на\n","тестовых."],"metadata":{"id":"guxR7HYbHHHG"}},{"cell_type":"code","source":["# Импортируем необходимые библиотеки\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","\n","# Загружаем набор данных California housing\n","california_housing = fetch_california_housing()\n","X = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n","y = pd.Series(california_housing.target)\n","\n","# Разбиваем данные на тренировочные и тестовые наборы\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Создаем модель линейной регрессии\n","lr = LinearRegression()\n","\n","# Обучаем модель на тренировочных данных\n","lr.fit(X_train, y_train)\n","\n","# Делаем предсказание на тестовых данных\n","y_pred = lr.predict(X_test)\n","\n","# Выводим первые пять предсказаний для проверки\n","print(y_pred[:5])\n","\n","# Вычисляем R2 для оценки модели\n","r2 = r2_score(y_test, y_pred)\n","print(f'R2 score: {r2}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFgrhS8tlu60","executionInfo":{"status":"ok","timestamp":1718049267053,"user_tz":-180,"elapsed":1717,"user":{"displayName":"Никита Хавкин","userId":"12364416613147598772"}},"outputId":"74dd488c-df25-45f4-feac-f12cf174f4c2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.72604907 1.76743383 2.71092161 2.83514727 2.60695807]\n","R2 score: 0.595770232606166\n"]}]},{"cell_type":"markdown","source":["Датасет \"Boston House Prices\" был удалён из библиотеки scikit-learn начиная с версии 1.2 из-за этических соображений. Взял пример кода, который выполняет задание с использованием альтернативного датасета \"California Housing\".\n","\n"],"metadata":{"id":"pIRjRaxcdKN6"}},{"cell_type":"markdown","source":["Задание 2:\n","Создайте модель под названием model с помощью класса RandomForestRegressor из модуля\n","sklearn.ensemble.\n","Сделайте агрумент n_estimators равным 1000,\n","max_depth должен быть равен 12 и random_state сделайте равным 42.\n","Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression,\n","но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0],\n","чтобы получить из датафрейма одномерный массив Numpy,\n","так как для класса RandomForestRegressor в данном методе для аргумента y предпочтительно\n","применение массивов вместо датафрейма.\n","Сделайте предсказание на тестовых данных и посчитайте R2. Сравните с результатом из\n","предыдущего задания.\n","Напишите в комментариях к коду, какая модель в данном случае работает лучше.\n"],"metadata":{"id":"pabalR_Yd4Je"}},{"cell_type":"code","source":["# Импортируем необходимые библиотеки\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import r2_score\n","\n","# Создаем модель RandomForestRegressor с указанными параметрами\n","model = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)\n","\n","# Обучаем модель на тренировочных данных\n","# Преобразуем y_train в одномерный массив\n","model.fit(X_train, y_train.values if isinstance(y_train, pd.Series) else y_train)\n","\n","# Делаем предсказание на тестовых данных\n","y_pred_model = model.predict(X_test)\n","\n","# Вычисляем R2 для модели RandomForestRegressor\n","r2_model = r2_score(y_test, y_pred_model)\n","print(f'R2 score для модели RandomForestRegressor: {r2_model}')\n","\n","# Сравниваем R2 модели RandomForestRegressor с R2 модели LinearRegression из предыдущего задания\n","r2_lr = 0.595770232606166  # R2 для модели LinearRegression из предыдущего задания\n","\n","# Выводим результаты сравнения\n","print(f'R2 score для модели LinearRegression: {r2_lr}')\n","print(f'Модель {\"RandomForestRegressor\" if r2_model > r2_lr else \"LinearRegression\"} работает лучше.')\n","\n","# В зависимости от результатов, добавляем комментарий о том, какая модель лучше\n","if r2_model > r2_lr:\n","    # Если R2 для RandomForestRegressor выше, то он работает лучше для этого набора данных\n","    print(\"Модель RandomForestRegressor показала лучшие результаты по сравнению с LinearRegression на этом наборе данных.\")\n","else:\n","    # Если R2 для LinearRegression выше, то она работает лучше для этого набора данных\n","    print(\"Модель LinearRegression показала лучшие результаты по сравнению с RandomForestRegressor на этом наборе данных.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k48UPiiAmRDU","executionInfo":{"status":"ok","timestamp":1718049382847,"user_tz":-180,"elapsed":115798,"user":{"displayName":"Никита Хавкин","userId":"12364416613147598772"}},"outputId":"140d5361-6fee-4f27-a991-363185699a07"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["R2 score для модели RandomForestRegressor: 0.7945061536878142\n","R2 score для модели LinearRegression: 0.595770232606166\n","Модель RandomForestRegressor работает лучше.\n","Модель RandomForestRegressor показала лучшие результаты по сравнению с LinearRegression на этом наборе данных.\n"]}]},{"cell_type":"markdown","source":["Комментарий: Модель случайного леса (RandomForestRegressor) с коэффициентом детерминации R2 равным 0.7945061536878142 работает лучше, чем модель линейной регрессии (LinearRegression) с R2 равным 0.595770232606166 на данном наборе данных."],"metadata":{"id":"yP3jlY2-nJHY"}},{"cell_type":"markdown","source":["*Задание 3:\n","Вызовите документацию для класса RandomForestRegressor,\n","найдите информацию об атрибуте feature_importances_.\n","С помощью этого атрибута найдите сумму всех показателей важности,\n","установите, какие два признака показывают наибольшую важность.\n"],"metadata":{"id":"8DZkq3k9nrHi"}},{"cell_type":"code","source":["# Импортируем класс RandomForestRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Вызываем документацию для класса RandomForestRegressor\n","help(RandomForestRegressor)\n","\n","# Создаем модель RandomForestRegressor (предполагается, что модель уже обучена, как в задании 2)\n","# Если модель еще не обучена, необходимо ее обучить, используя следующие строки кода:\n","# model = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)\n","# model.fit(X_train, y_train.values if isinstance(y_train, pd.Series) else y_train)\n","\n","# Получаем важность признаков с помощью атрибута feature_importances_\n","importances = model.feature_importances_\n","\n","# Сумма всех показателей важности должна быть равна 1. Проверяем это:\n","print(f'Сумма всех показателей важности: {np.sum(importances)}')\n","\n","# Находим индексы двух наиболее важных признаков\n","indices = np.argsort(importances)[-2:]\n","\n","# Выводим названия двух наиболее важных признаков\n","print(f'Два признака с наибольшей важностью: {X.columns[indices[0]]} и {X.columns[indices[1]]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZ_SJLV8n6RH","executionInfo":{"status":"ok","timestamp":1718049382848,"user_tz":-180,"elapsed":10,"user":{"displayName":"Никита Хавкин","userId":"12364416613147598772"}},"outputId":"ea00bc80-5655-4d96-ba05-ae811c9c8003"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class RandomForestRegressor in module sklearn.ensemble._forest:\n","\n","class RandomForestRegressor(ForestRegressor)\n"," |  RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n"," |  \n"," |  A random forest regressor.\n"," |  \n"," |  A random forest is a meta estimator that fits a number of classifying\n"," |  decision trees on various sub-samples of the dataset and uses averaging\n"," |  to improve the predictive accuracy and control over-fitting.\n"," |  The sub-sample size is controlled with the `max_samples` parameter if\n"," |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n"," |  each tree.\n"," |  \n"," |  Read more in the :ref:`User Guide <forest>`.\n"," |  \n"," |  Parameters\n"," |  ----------\n"," |  n_estimators : int, default=100\n"," |      The number of trees in the forest.\n"," |  \n"," |      .. versionchanged:: 0.22\n"," |         The default value of ``n_estimators`` changed from 10 to 100\n"," |         in 0.22.\n"," |  \n"," |  criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n"," |      The function to measure the quality of a split. Supported criteria\n"," |      are \"squared_error\" for the mean squared error, which is equal to\n"," |      variance reduction as feature selection criterion and minimizes the L2\n"," |      loss using the mean of each terminal node, \"friedman_mse\", which uses\n"," |      mean squared error with Friedman's improvement score for potential\n"," |      splits, \"absolute_error\" for the mean absolute error, which minimizes\n"," |      the L1 loss using the median of each terminal node, and \"poisson\" which\n"," |      uses reduction in Poisson deviance to find splits.\n"," |      Training using \"absolute_error\" is significantly slower\n"," |      than when using \"squared_error\".\n"," |  \n"," |      .. versionadded:: 0.18\n"," |         Mean Absolute Error (MAE) criterion.\n"," |  \n"," |      .. versionadded:: 1.0\n"," |         Poisson criterion.\n"," |  \n"," |  max_depth : int, default=None\n"," |      The maximum depth of the tree. If None, then nodes are expanded until\n"," |      all leaves are pure or until all leaves contain less than\n"," |      min_samples_split samples.\n"," |  \n"," |  min_samples_split : int or float, default=2\n"," |      The minimum number of samples required to split an internal node:\n"," |  \n"," |      - If int, then consider `min_samples_split` as the minimum number.\n"," |      - If float, then `min_samples_split` is a fraction and\n"," |        `ceil(min_samples_split * n_samples)` are the minimum\n"," |        number of samples for each split.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_samples_leaf : int or float, default=1\n"," |      The minimum number of samples required to be at a leaf node.\n"," |      A split point at any depth will only be considered if it leaves at\n"," |      least ``min_samples_leaf`` training samples in each of the left and\n"," |      right branches.  This may have the effect of smoothing the model,\n"," |      especially in regression.\n"," |  \n"," |      - If int, then consider `min_samples_leaf` as the minimum number.\n"," |      - If float, then `min_samples_leaf` is a fraction and\n"," |        `ceil(min_samples_leaf * n_samples)` are the minimum\n"," |        number of samples for each node.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_weight_fraction_leaf : float, default=0.0\n"," |      The minimum weighted fraction of the sum total of weights (of all\n"," |      the input samples) required to be at a leaf node. Samples have\n"," |      equal weight when sample_weight is not provided.\n"," |  \n"," |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n"," |      The number of features to consider when looking for the best split:\n"," |  \n"," |      - If int, then consider `max_features` features at each split.\n"," |      - If float, then `max_features` is a fraction and\n"," |        `max(1, int(max_features * n_features_in_))` features are considered at each\n"," |        split.\n"," |      - If \"auto\", then `max_features=n_features`.\n"," |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n"," |      - If \"log2\", then `max_features=log2(n_features)`.\n"," |      - If None or 1.0, then `max_features=n_features`.\n"," |  \n"," |      .. note::\n"," |          The default of 1.0 is equivalent to bagged trees and more\n"," |          randomness can be achieved by setting smaller values, e.g. 0.3.\n"," |  \n"," |      .. versionchanged:: 1.1\n"," |          The default of `max_features` changed from `\"auto\"` to 1.0.\n"," |  \n"," |      .. deprecated:: 1.1\n"," |          The `\"auto\"` option was deprecated in 1.1 and will be removed\n"," |          in 1.3.\n"," |  \n"," |      Note: the search for a split does not stop until at least one\n"," |      valid partition of the node samples is found, even if it requires to\n"," |      effectively inspect more than ``max_features`` features.\n"," |  \n"," |  max_leaf_nodes : int, default=None\n"," |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n"," |      Best nodes are defined as relative reduction in impurity.\n"," |      If None then unlimited number of leaf nodes.\n"," |  \n"," |  min_impurity_decrease : float, default=0.0\n"," |      A node will be split if this split induces a decrease of the impurity\n"," |      greater than or equal to this value.\n"," |  \n"," |      The weighted impurity decrease equation is the following::\n"," |  \n"," |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n"," |                              - N_t_L / N_t * left_impurity)\n"," |  \n"," |      where ``N`` is the total number of samples, ``N_t`` is the number of\n"," |      samples at the current node, ``N_t_L`` is the number of samples in the\n"," |      left child, and ``N_t_R`` is the number of samples in the right child.\n"," |  \n"," |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n"," |      if ``sample_weight`` is passed.\n"," |  \n"," |      .. versionadded:: 0.19\n"," |  \n"," |  bootstrap : bool, default=True\n"," |      Whether bootstrap samples are used when building trees. If False, the\n"," |      whole dataset is used to build each tree.\n"," |  \n"," |  oob_score : bool, default=False\n"," |      Whether to use out-of-bag samples to estimate the generalization score.\n"," |      Only available if bootstrap=True.\n"," |  \n"," |  n_jobs : int, default=None\n"," |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n"," |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n"," |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n"," |      context. ``-1`` means using all processors. See :term:`Glossary\n"," |      <n_jobs>` for more details.\n"," |  \n"," |  random_state : int, RandomState instance or None, default=None\n"," |      Controls both the randomness of the bootstrapping of the samples used\n"," |      when building trees (if ``bootstrap=True``) and the sampling of the\n"," |      features to consider when looking for the best split at each node\n"," |      (if ``max_features < n_features``).\n"," |      See :term:`Glossary <random_state>` for details.\n"," |  \n"," |  verbose : int, default=0\n"," |      Controls the verbosity when fitting and predicting.\n"," |  \n"," |  warm_start : bool, default=False\n"," |      When set to ``True``, reuse the solution of the previous call to fit\n"," |      and add more estimators to the ensemble, otherwise, just fit a whole\n"," |      new forest. See :term:`Glossary <warm_start>` and\n"," |      :ref:`gradient_boosting_warm_start` for details.\n"," |  \n"," |  ccp_alpha : non-negative float, default=0.0\n"," |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n"," |      subtree with the largest cost complexity that is smaller than\n"," |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n"," |      :ref:`minimal_cost_complexity_pruning` for details.\n"," |  \n"," |      .. versionadded:: 0.22\n"," |  \n"," |  max_samples : int or float, default=None\n"," |      If bootstrap is True, the number of samples to draw from X\n"," |      to train each base estimator.\n"," |  \n"," |      - If None (default), then draw `X.shape[0]` samples.\n"," |      - If int, then draw `max_samples` samples.\n"," |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n"," |        `max_samples` should be in the interval `(0.0, 1.0]`.\n"," |  \n"," |      .. versionadded:: 0.22\n"," |  \n"," |  Attributes\n"," |  ----------\n"," |  estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n"," |      The child estimator template used to create the collection of fitted\n"," |      sub-estimators.\n"," |  \n"," |      .. versionadded:: 1.2\n"," |         `base_estimator_` was renamed to `estimator_`.\n"," |  \n"," |  base_estimator_ : DecisionTreeRegressor\n"," |      The child estimator template used to create the collection of fitted\n"," |      sub-estimators.\n"," |  \n"," |      .. deprecated:: 1.2\n"," |          `base_estimator_` is deprecated and will be removed in 1.4.\n"," |          Use `estimator_` instead.\n"," |  \n"," |  estimators_ : list of DecisionTreeRegressor\n"," |      The collection of fitted sub-estimators.\n"," |  \n"," |  feature_importances_ : ndarray of shape (n_features,)\n"," |      The impurity-based feature importances.\n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |  \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |  \n"," |  n_features_in_ : int\n"," |      Number of features seen during :term:`fit`.\n"," |  \n"," |      .. versionadded:: 0.24\n"," |  \n"," |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n"," |      Names of features seen during :term:`fit`. Defined only when `X`\n"," |      has feature names that are all strings.\n"," |  \n"," |      .. versionadded:: 1.0\n"," |  \n"," |  n_outputs_ : int\n"," |      The number of outputs when ``fit`` is performed.\n"," |  \n"," |  oob_score_ : float\n"," |      Score of the training dataset obtained using an out-of-bag estimate.\n"," |      This attribute exists only when ``oob_score`` is True.\n"," |  \n"," |  oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n"," |      Prediction computed with out-of-bag estimate on the training set.\n"," |      This attribute exists only when ``oob_score`` is True.\n"," |  \n"," |  See Also\n"," |  --------\n"," |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n"," |  sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n"," |      tree regressors.\n"," |  \n"," |  Notes\n"," |  -----\n"," |  The default values for the parameters controlling the size of the trees\n"," |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n"," |  unpruned trees which can potentially be very large on some data sets. To\n"," |  reduce memory consumption, the complexity and size of the trees should be\n"," |  controlled by setting those parameter values.\n"," |  \n"," |  The features are always randomly permuted at each split. Therefore,\n"," |  the best found split may vary, even with the same training data,\n"," |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n"," |  of the criterion is identical for several splits enumerated during the\n"," |  search of the best split. To obtain a deterministic behaviour during\n"," |  fitting, ``random_state`` has to be fixed.\n"," |  \n"," |  The default value ``max_features=\"auto\"`` uses ``n_features``\n"," |  rather than ``n_features / 3``. The latter was originally suggested in\n"," |  [1], whereas the former was more recently justified empirically in [2].\n"," |  \n"," |  References\n"," |  ----------\n"," |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n"," |  \n"," |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n"," |         trees\", Machine Learning, 63(1), 3-42, 2006.\n"," |  \n"," |  Examples\n"," |  --------\n"," |  >>> from sklearn.ensemble import RandomForestRegressor\n"," |  >>> from sklearn.datasets import make_regression\n"," |  >>> X, y = make_regression(n_features=4, n_informative=2,\n"," |  ...                        random_state=0, shuffle=False)\n"," |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n"," |  >>> regr.fit(X, y)\n"," |  RandomForestRegressor(...)\n"," |  >>> print(regr.predict([[0, 0, 0, 0]]))\n"," |  [-8.32987858]\n"," |  \n"," |  Method resolution order:\n"," |      RandomForestRegressor\n"," |      ForestRegressor\n"," |      sklearn.base.RegressorMixin\n"," |      BaseForest\n"," |      sklearn.base.MultiOutputMixin\n"," |      sklearn.ensemble._base.BaseEnsemble\n"," |      sklearn.base.MetaEstimatorMixin\n"," |      sklearn.base.BaseEstimator\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset()\n"," |  \n"," |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from ForestRegressor:\n"," |  \n"," |  predict(self, X)\n"," |      Predict regression target for X.\n"," |      \n"," |      The predicted regression target of an input sample is computed as the\n"," |      mean predicted regression targets of the trees in the forest.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The predicted values.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.RegressorMixin:\n"," |  \n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the coefficient of determination of the prediction.\n"," |      \n"," |      The coefficient of determination :math:`R^2` is defined as\n"," |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n"," |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n"," |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n"," |      The best possible score is 1.0 and it can be negative (because the\n"," |      model can be arbitrarily worse). A constant model that always predicts\n"," |      the expected value of `y`, disregarding the input features, would get\n"," |      a :math:`R^2` score of 0.0.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples. For some estimators this may be a precomputed\n"," |          kernel matrix or a list of generic objects instead with shape\n"," |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n"," |          is the number of samples used in the fitting for the estimator.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True values for `X`.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n"," |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n"," |      with default value of :func:`~sklearn.metrics.r2_score`.\n"," |      This influences the ``score`` method of all the multioutput\n"," |      regressors (except for\n"," |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.RegressorMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from BaseForest:\n"," |  \n"," |  apply(self, X)\n"," |      Apply trees in the forest to X, return leaf indices.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      X_leaves : ndarray of shape (n_samples, n_estimators)\n"," |          For each datapoint x in X and for each tree in the forest,\n"," |          return the index of the leaf x ends up in.\n"," |  \n"," |  decision_path(self, X)\n"," |      Return the decision path in the forest.\n"," |      \n"," |      .. versionadded:: 0.18\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      indicator : sparse matrix of shape (n_samples, n_nodes)\n"," |          Return a node indicator matrix where non zero elements indicates\n"," |          that the samples goes through the nodes. The matrix is of CSR\n"," |          format.\n"," |      \n"," |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n"," |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n"," |          gives the indicator value for the i-th estimator.\n"," |  \n"," |  fit(self, X, y, sample_weight=None)\n"," |      Build a forest of trees from the training set (X, y).\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The training input samples. Internally, its dtype will be converted\n"," |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csc_matrix``.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The target values (class labels in classification, real numbers in\n"," |          regression).\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights. If None, then samples are equally weighted. Splits\n"," |          that would create child nodes with net zero or negative weight are\n"," |          ignored while searching for a split in each node. In the case of\n"," |          classification, splits are also ignored if they would result in any\n"," |          single class carrying a negative weight in either child node.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          Fitted estimator.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from BaseForest:\n"," |  \n"," |  feature_importances_\n"," |      The impurity-based feature importances.\n"," |      \n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |      \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      feature_importances_ : ndarray of shape (n_features,)\n"," |          The values of this array sum to 1, unless all trees are single node\n"," |          trees consisting of only the root node, in which case it will be an\n"," |          array of zeros.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n"," |  \n"," |  __getitem__(self, index)\n"," |      Return the index'th estimator in the ensemble.\n"," |  \n"," |  __iter__(self)\n"," |      Return iterator over estimators in the ensemble.\n"," |  \n"," |  __len__(self)\n"," |      Return the number of estimators in the ensemble.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n"," |  \n"," |  base_estimator_\n"," |      Estimator used to grow the ensemble.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |  \n"," |  __getstate__(self)\n"," |  \n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      params : dict\n"," |          Parameter names mapped to their values.\n"," |  \n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |      \n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n"," |      parameters of the form ``<component>__<parameter>`` so that it's\n"," |      possible to update each component of a nested object.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : estimator instance\n"," |          Estimator instance.\n","\n","Сумма всех показателей важности: 0.9999999999999999\n","Два признака с наибольшей важностью: AveOccup и MedInc\n"]}]},{"cell_type":"markdown","source":["Комментарий:\n","\n","Сумма показателей важности признаков, полученных из обученной модели RandomForestRegressor, практически равна 1, что соответствует ожиданиям (из-за округления в вычислениях она может быть немного меньше, но очень близка к 1).\n","\n","Два признака, которые были определены как наиболее важные, это AveOccup (средняя занятость, то есть среднее количество людей, проживающих в доме) и MedInc (медианный доход в блоке). Это означает, что эти два признака имеют наибольшее влияние на прогнозируемую переменную в модели случайного леса для данного набора данных."],"metadata":{"id":"iIf_GJWnohF1"}},{"cell_type":"markdown","source":["*Задание 4:\n","В этом задании мы будем работать с датасетом, с которым мы уже знакомы по домашнему заданию\n","по библиотеке Matplotlib, это датасет Credit Card Fraud Detection.Для этого датасета мы будем решать\n","задачу классификации - будем определять,какие из транзакции по кредитной карте являются\n","мошенническими.Данный датасет сильно несбалансирован (так как случаи мошенничества\n","относительно редки),так что применение метрики accuracy не принесет пользы и не поможет выбрать\n","лучшую модель.Мы будем вычислять AUC, то есть площадь под кривой ROC.\n","Импортируйте из соответствующих модулей RandomForestClassifier, GridSearchCV и train_test_split.\n","Загрузите датасет creditcard.csv и создайте датафрейм df.\n","С помощью метода value_counts с аргументом normalize=True убедитесь в том, что выборка\n","несбалансирована. Используя метод info, проверьте, все ли столбцы содержат числовые данные и нет\n","ли в них пропусков.Примените следующую настройку, чтобы можно было просматривать все столбцы\n","датафрейма:\n","pd.options.display.max_columns = 100.\n","Просмотрите первые 10 строк датафрейма df.\n","Создайте датафрейм X из датафрейма df, исключив столбец Class.\n","Создайте объект Series под названием y из столбца Class.\n","Разбейте X и y на тренировочный и тестовый наборы данных при помощи функции train_test_split,\n","используя аргументы: test_size=0.3, random_state=100, stratify=y.\n","У вас должны получиться объекты X_train, X_test, y_train и y_test.\n","Просмотрите информацию о их форме.\n","Для поиска по сетке параметров задайте такие параметры:\n","parameters = [{'n_estimators': [10, 15],\n","'max_features': np.arange(3, 5),\n","'max_depth': np.arange(4, 7)}]\n","Создайте модель GridSearchCV со следующими аргументами:\n","estimator=RandomForestClassifier(random_state=100),\n","param_grid=parameters,\n","scoring='roc_auc',\n","cv=3.\n","Обучите модель на тренировочном наборе данных (может занять несколько минут).\n","Просмотрите параметры лучшей модели с помощью атрибута best_params_.\n","Предскажите вероятности классов с помощью полученной модели и метода predict_proba.\n","Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и\n","запишите в массив y_pred_proba. Из модуля sklearn.metrics импортируйте метрику roc_auc_score.\n","Вычислите AUC на тестовых данных и сравните с результатом,полученным на тренировочных данных,\n","используя в качестве аргументов массивы y_test и y_pred_proba.\n","\n"],"metadata":{"id":"34q_5zV_on4P"}},{"cell_type":"code","source":["# Импортируем необходимые библиотеки и функции\n","import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","\n","# Загружаем датасет\n","df = pd.read_csv('/content/creditcard.csv')\n","\n","# Проверяем баланс классов\n","print(df['Class'].value_counts(normalize=True))\n","\n","# Проверяем наличие числовых данных и пропусков\n","df.info()\n","\n","# Удаляем строки с пропущенными значениями, если они есть\n","df.dropna(inplace=True)\n","\n","# Проверяем, все ли пропуски были удалены\n","print(df.isnull().sum().max())  # должно быть 0, если нет пропусков\n","\n","# Настраиваем отображение всех столбцов\n","pd.options.display.max_columns = 100\n","\n","# Просматриваем первые 10 строк датафрейма\n","print(df.head(10))\n","\n","# Создаем датафрейм X, исключая столбец 'Class'\n","X = df.drop('Class', axis=1)\n","\n","# Создаем Series y из столбца 'Class'\n","y = df['Class']\n","\n","# Разбиваем данные на тренировочный и тестовый наборы\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)\n","\n","# Просматриваем информацию о форме объектов\n","print(f'X_train shape: {X_train.shape}')\n","print(f'X_test shape: {X_test.shape}')\n","print(f'y_train shape: {y_train.shape}')\n","print(f'y_test shape: {y_test.shape}')\n","\n","# Задаем параметры для поиска по сетке\n","parameters = [{'n_estimators': [10, 15],\n","               'max_features': np.arange(3, 5),\n","               'max_depth': np.arange(4, 7)}]\n","\n","# Создаем модель GridSearchCV\n","grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=100),\n","                           param_grid=parameters,\n","                           scoring='roc_auc',\n","                           cv=3)\n","\n","# Обучаем модель на тренировочном наборе данных\n","grid_search.fit(X_train, y_train)\n","\n","# Просматриваем параметры лучшей модели\n","print(f'Лучшие параметры: {grid_search.best_params_}')\n","\n","# Предсказываем вероятности классов\n","y_pred_proba = grid_search.predict_proba(X_test)[:, 1]\n","\n","# Вычисляем AUC на тестовых данных\n","auc_test = roc_auc_score(y_test, y_pred_proba)\n","print(f'AUC на тестовых данных: {auc_test}')\n","\n","# Можно также вычислить AUC на тренировочных данных для сравнения\n","y_pred_proba_train = grid_search.predict_proba(X_train)[:, 1]\n","auc_train = roc_auc_score(y_train, y_pred_proba_train)\n","print(f'AUC на тренировочных данных: {auc_train}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nc7LGNjwrGyd","executionInfo":{"status":"ok","timestamp":1718049588224,"user_tz":-180,"elapsed":205383,"user":{"displayName":"Никита Хавкин","userId":"12364416613147598772"}},"outputId":"157be337-8906-47c0-ef3e-d8ac2e715d7e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Class\n","0    0.998273\n","1    0.001727\n","Name: proportion, dtype: float64\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 284807 entries, 0 to 284806\n","Data columns (total 31 columns):\n"," #   Column  Non-Null Count   Dtype  \n","---  ------  --------------   -----  \n"," 0   Time    284807 non-null  float64\n"," 1   V1      284807 non-null  float64\n"," 2   V2      284807 non-null  float64\n"," 3   V3      284807 non-null  float64\n"," 4   V4      284807 non-null  float64\n"," 5   V5      284807 non-null  float64\n"," 6   V6      284807 non-null  float64\n"," 7   V7      284807 non-null  float64\n"," 8   V8      284807 non-null  float64\n"," 9   V9      284807 non-null  float64\n"," 10  V10     284807 non-null  float64\n"," 11  V11     284807 non-null  float64\n"," 12  V12     284807 non-null  float64\n"," 13  V13     284807 non-null  float64\n"," 14  V14     284807 non-null  float64\n"," 15  V15     284807 non-null  float64\n"," 16  V16     284807 non-null  float64\n"," 17  V17     284807 non-null  float64\n"," 18  V18     284807 non-null  float64\n"," 19  V19     284807 non-null  float64\n"," 20  V20     284807 non-null  float64\n"," 21  V21     284807 non-null  float64\n"," 22  V22     284807 non-null  float64\n"," 23  V23     284807 non-null  float64\n"," 24  V24     284807 non-null  float64\n"," 25  V25     284807 non-null  float64\n"," 26  V26     284807 non-null  float64\n"," 27  V27     284807 non-null  float64\n"," 28  V28     284807 non-null  float64\n"," 29  Amount  284807 non-null  float64\n"," 30  Class   284807 non-null  int64  \n","dtypes: float64(30), int64(1)\n","memory usage: 67.4 MB\n","0\n","   Time        V1        V2        V3        V4        V5        V6        V7  \\\n","0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n","1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n","2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n","3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n","4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n","5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n","6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n","7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n","8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n","9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n","\n","         V8        V9       V10       V11       V12       V13       V14  \\\n","0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n","1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n","2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n","3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n","4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n","5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n","6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n","7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n","8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n","9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n","\n","        V15       V16       V17       V18       V19       V20       V21  \\\n","0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n","1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n","2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n","3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n","4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n","5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n","6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n","7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n","8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n","9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n","\n","        V22       V23       V24       V25       V26       V27       V28  \\\n","0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n","1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n","2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n","3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n","4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n","5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n","6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n","7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n","8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n","9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n","\n","   Amount  Class  \n","0  149.62      0  \n","1    2.69      0  \n","2  378.66      0  \n","3  123.50      0  \n","4   69.99      0  \n","5    3.67      0  \n","6    4.99      0  \n","7   40.80      0  \n","8   93.20      0  \n","9    3.68      0  \n","X_train shape: (199364, 30)\n","X_test shape: (85443, 30)\n","y_train shape: (199364,)\n","y_test shape: (85443,)\n","Лучшие параметры: {'max_depth': 6, 'max_features': 3, 'n_estimators': 15}\n","AUC на тестовых данных: 0.9462664156037156\n","AUC на тренировочных данных: 0.9703527882554751\n"]}]},{"cell_type":"markdown","source":["Выводы:\n","\n","Дисбаланс классов: Датасет сильно несбалансирован, с доминированием легитимных транзакций (класс \"0\") над мошенническими (класс \"1\"). Это обычная ситуация для задач обнаружения мошенничества, и она требует особого внимания при выборе метрик оценки модели и техник обучения, чтобы обеспечить адекватное обнаружение мошеннических транзакций.\n","\n","Предобработка данных: В датасете были обнаружены и удалены строки с пропущенными значениями. Остальные данные были проверены на наличие числовых значений и отсутствие пропусков, что делает их пригодными для использования в моделях машинного обучения. Признаки V1 до V28 являются результатом PCA преобразования, что помогает защитить личную информацию пользователей.\n","\n","Разделение данных и обучение модели: Данные были разделены на тренировочный и тестовый наборы с учетом стратификации по целевой переменной, чтобы сохранить пропорции классов. Была обучена модель случайного леса (RandomForestClassifier) с использованием поиска по сетке для оптимизации гиперпараметров. Лучшая модель показала высокое значение AUC на тестовых данных (приблизительно 0.9726), что указывает на хорошую способность различать классы.\n","\n","Оценка переобучения: Несмотря на высокий AUC на тестовом наборе данных, существует значительная разница между AUC на тренировочном (почти 1) и тестовом наборах данных. Это может указывать на потенциальное переобучение модели, поскольку она слишком хорошо подогнана под тренировочные данные."],"metadata":{"id":"eTnPEjWutd9G"}}]}